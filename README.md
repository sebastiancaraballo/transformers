# ğŸ¤— Hugging Face LLM Course Practical Exercise

This repository contains a comprehensive practical exercise covering Chapters 1-4 of the Hugging Face LLM course, with a **corrected version** that fixes common dataset sampling issues.

## ğŸ“š Exercise Overview

**Goal**: Train and Share Your First Transformer

By completing this exercise, you will apply the concepts from Chapters 1â€“4:

- **Chapter 1-2**: Use pretrained Transformer models for text classification
- **Chapter 3**: Fine-tune models on custom datasets  
- **Chapter 4**: Share models publicly on the Hugging Face Hub

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8+
- Jupyter Notebook or JupyterLab
- Hugging Face account (for sharing models)

### Installation

1. **Clone this repository:**
   ```bash
   git clone https://github.com/sebastiancaraballo/transformers.git
   cd transformers
   ```

2. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

3. **Start Jupyter:**
   ```bash
   jupyter notebook
   ```

4. **Open the CORRECTED notebook:**
   - Open `huggingface_practical_exercise_corrected.ipynb` â­
   - Follow the step-by-step instructions

## ğŸ“– What You'll Learn

### ğŸ”¹ Chapter 1-2: Using Pretrained Models
- Load sentiment analysis models with the `pipeline` API
- Make predictions on custom text
- Understand model confidence scores

### ğŸ”¹ Chapter 3: Fine-tuning Models
- Load and prepare the IMDb dataset with **balanced sampling**
- Tokenize text data
- Fine-tune DistilBERT for sentiment analysis
- Evaluate model performance
- **Debug common training issues**

### ğŸ”¹ Chapter 4: Sharing on the Hub
- Save models locally
- Push models to Hugging Face Hub
- Make your model publicly available

## ğŸ¯ Expected Outcomes

After completing this exercise, you will have:
- A **properly functioning** fine-tuned sentiment analysis model
- Experience with the complete ML workflow
- A published model on Hugging Face Hub
- Practical knowledge of transformer architectures
- **Understanding of common pitfalls and how to avoid them**

## ğŸ“ Repository Structure

```
transformers/
â”œâ”€â”€ huggingface_practical_exercise_corrected.ipynb  # âœ… RECOMMENDED - Fixed version
â”œâ”€â”€ huggingface_practical_exercise.ipynb            # âš ï¸ Original version (has issues)
â”œâ”€â”€ requirements.txt                                 # Python dependencies
â”œâ”€â”€ README.md                                       # This file
â”œâ”€â”€ my-fine-tuned-sentiment-model/                  # Saved model (if you run the exercise)
â”œâ”€â”€ results/                                        # Training results (if you run the exercise)
â””â”€â”€ .gitignore                                     # Git ignore file
```

## ğŸ”§ Customization

Feel free to modify the exercise:
- Change the dataset (try other text classification tasks)
- Experiment with different model architectures
- Adjust training parameters
- Use different evaluation metrics

## ğŸ› Common Issues and Solutions

### Issue: Model predicts everything as the same class
**Solution**: Check dataset balance and label mapping

### Issue: 100% accuracy on evaluation
**Solution**: Usually indicates overfitting or dataset issues

### Issue: Poor performance
**Solution**: Try more training data or different hyperparameters

### Issue: Label mismatch
**Solution**: Ensure your model's label mapping matches the dataset

## ğŸ“š Additional Resources

- [Hugging Face Course](https://huggingface.co/course)
- [Transformers Documentation](https://huggingface.co/docs/transformers/)
- [Datasets Documentation](https://huggingface.co/docs/datasets/)
- [Model Hub](https://huggingface.co/models)

## ğŸ¤ Contributing

Feel free to submit issues, feature requests, or pull requests to improve this exercise!

## ğŸ“„ License

This project is open source and available under the [MIT License](LICENSE).

---

**Happy Learning! ğŸ‰**

*Remember: Always use the corrected notebook for the best experience!*
